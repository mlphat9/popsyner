{"cells":[{"cell_type":"markdown","metadata":{"id":"rXvWz-IMMQzy"},"source":["# Import libraries"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"q9q3I5LH5LSJ"},"outputs":[],"source":["import matplotlib.pyplot as plt\n","import multiprocessing\n","from multiprocessing import Process, Queue\n","import numpy as np\n","import pandas as pd\n","import sklearn.preprocessing as skp\n","import time\n","import torch\n","import torch.nn as nn\n","from torch.nn import functional as F\n","from imblearn.over_sampling import SMOTENC\n","import threading\n","import json"]},{"cell_type":"markdown","source":["# Global variables"],"metadata":{"id":"xQVw927V7nrE"}},{"cell_type":"code","source":["MODEL_CONFIG = 'model/config.json'\n","\n","DATA_PATH = 'data/raw_data.csv'\n","\n","# ------------- Train, Test, Validation Set ----------- #\n","TEST_DF_PATH = 'data/test_df.csv'\n","TRAIN_DF_PATH = 'data/train_df.csv'\n","VALID_DF_PATH = 'data/valid_df.csv'\n","\n","H_POP_PATH = 'data/h_population.csv'\n","\n","# ------------------- Model Training ----------------- #\n","device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","random_state = 2023\n","torch.manual_seed(random_state)\n","np.random.seed(random_state)\n","debug = False\n","MODEL_PATH = 'model/tfm.pt'"],"metadata":{"id":"Nq966rJvTSHt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["MAX_ITERS = 1000 # @param {type:\"slider\", min:100, max:2000, step:100}\n","GENERATED_SIZE = 1000 # @param {type:\"slider\", min:1000, max:100000, step:500}\n"],"metadata":{"id":"p-_kyoDufPry"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Load model config\n","with open(MODEL_CONFIG, 'r') as openfile:\n","    prm_json = json.load(openfile)\n","\n","batch_size = prm_json[\"batch_size\"]\n","block_size = prm_json[\"block_size\"]\n","eval_interval = prm_json[\"eval_interval\"]\n","learning_rate = prm_json[\"learning_rate\"]\n","eval_iters = prm_json[\"eval_iters\"]\n","n_embd = prm_json[\"n_embd\"]\n","n_head = prm_json[\"n_head\"]\n","n_layer = prm_json[\"n_layer\"]\n","dropout = prm_json[\"dropout\"]\n","vocab_size = prm_json[\"vocab_size\"]\n","max_vocab = prm_json[\"max_vocab\"]\n","\n","\n","print(f\"\"\"batch_size: {batch_size}, block_size: {block_size},\n","eval_interval: {eval_interval}, learning_rate: {learning_rate},\n","eval_iters: {eval_iters}, n_embd: {n_embd},\n","n_head: {n_head}, n_layer: {n_layer}, dropout: {dropout},\n","vocab_size: {vocab_size}, max_vocab: {max_vocab}\"\"\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1719906420398,"user_tz":-420,"elapsed":5,"user":{"displayName":"Phattranit Phattharajiranan","userId":"00695574895698891028"}},"outputId":"6440fc6b-ad47-41b7-d826-5e4dec695c22","id":"DLwDTYyeZj7D"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["batch_size: 32, block_size: 4,\n","eval_interval: 10, learning_rate: 0.0001,\n","eval_iters: 10, n_embd: 512,\n","n_head: 8, n_layer: 6, dropout: 0.1,\n","vocab_size: 112, max_vocab: 409\n"]}]},{"cell_type":"markdown","metadata":{"id":"7jIbxTJbEjpG"},"source":["# Util Functions"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EurQjjD4S4qO"},"outputs":[],"source":["def binning(df, column, bin_edges, labels=None):\n","    \"\"\"\n","    Function that performs binning for a numeric column in a Pandas dataframe.\n","    df: Pandas dataframe\n","    column: name of the numeric column to be binned\n","    bin_edges: list of bin edges to use\n","    labels: list of labels to assign to the bins\n","    \"\"\"\n","    # Use the Pandas cut function to bin the column\n","    df[column + '_binned'] = pd.cut(df[column], bins=bin_edges, labels=labels)\n","\n","    return df"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PPFCeIsUJqXF"},"outputs":[],"source":["def compare_attributes(df1, df2, columns_to_plot=None, legend_labels=None):\n","    if columns_to_plot is None:\n","        columns_to_plot = df1.columns\n","\n","    # Iterate over each column in the dataframes\n","    for i, col in enumerate(columns_to_plot):\n","        # Create a new figure and axis object for each attribute\n","        fig, ax = plt.subplots()\n","\n","        # Get the frequencies for each unique value in the column\n","        freq1 = df1[col].value_counts()\n","        freq2 = df2[col].value_counts()\n","\n","        # Combine the frequencies into a single dataframe\n","        freq_df = pd.concat([freq1, freq2], axis=1, keys=['df1', 'df2'])\n","\n","        # Plot the frequencies as a bar chart\n","        freq_df.plot(kind='bar', ax=ax, rot=0)\n","\n","        # Set the title for the subplot\n","        ax.set_title(col)\n","\n","        # Add a legend to the plot\n","        plt.legend(legend_labels)\n","\n","        # Show the plot\n","        plt.show()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jdk58btmyQC5"},"outputs":[],"source":["def count_unique_elements(my_list):\n","    # Create an empty list to store unique elements\n","    unique_list = []\n","\n","    # Loop through each element in the list\n","    for element in my_list:\n","        # Check if the element is not already in the unique list\n","        if element not in unique_list:\n","            # If it's not, add it to the unique list\n","            unique_list.append(element)\n","            unique_list.sort()\n","            max_vocab = max(unique_list)\n","\n","    # Return the length of the unique list and the unique list itself\n","    return len(unique_list), unique_list, max_vocab"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DSxhNCBPUiKF"},"outputs":[],"source":["# Convert the model output into a pandas dataframe\n","def create_dataframe(data_list, num_columns, header=None):\n","    # Calculate the number of rows needed based on the length of the data_list\n","    # and the specified number of columns\n","    num_rows = len(data_list) // num_columns + (len(data_list) % num_columns > 0)\n","\n","    # Create a 2D array with the data_list values\n","    data_array = [data_list[i:i+num_columns] for i in range(0, len(data_list), num_columns)]\n","\n","    # Add empty values to the end of each row to ensure all rows have the same length\n","    for i in range(len(data_array)):\n","        while len(data_array[i]) < num_columns:\n","            data_array[i].append(None)\n","\n","    # Create a Pandas dataframe from the 2D array\n","    df = pd.DataFrame(data_array, columns=header)\n","\n","    return df\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jtjaB-OIMXvo"},"outputs":[],"source":["def flatten_list(nested_list):\n","    flat_list = []\n","    for inner_list in nested_list:\n","        for element in inner_list:\n","            flat_list.append(element)\n","\n","    return flat_list"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gqbF2hd9RW7j"},"outputs":[],"source":["## Train and Test Splits\n","# data loading\n","def get_batch(split):\n","    # generate a small batch of data of inputs x and targets y\n","    data = train_data if split == 'train' else val_data\n","    ix = torch.randint(len(data) - block_size, (batch_size,))\n","\n","    if debug == True:\n","        print(f\"ix: {ix}\")\n","\n","    x = torch.stack([data[i:i+block_size] for i in ix])\n","\n","    if debug == True:\n","        print(f\"x: {x}\")\n","\n","    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n","\n","    if debug == True:\n","        print(f\"y: {y}\")\n","\n","    x, y = x.to(device), y.to(device)\n","    return x, y\n","\n","@torch.no_grad()\n","def estimate_loss():\n","    out = {}\n","    model.eval()\n","    for split in ['train', 'val']:\n","        losses = torch.zeros(eval_iters)\n","        for k in range(eval_iters):\n","\n","            if debug == True:\n","                print(f\"k: {k}\")\n","\n","            X, Y = get_batch(split)\n","\n","            if debug == True:\n","                print(f\"X: {X}\")\n","                print(f\"Y: {Y}\")\n","\n","            logits, loss = model(X, Y)\n","\n","            if debug == True:\n","                print(f\"logits: {logits}\")\n","                print(f\"loss: {loss}\")\n","\n","            losses[k] = loss.item()\n","        out[split] = losses.mean()\n","    model.train()\n","    return out"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ysAuD2wHEMMC"},"outputs":[],"source":["def plot_attribute_distribution(df):\n","    # check the distribution of each attribute\n","    # iterate over columns in dataframe\n","    for col in df.columns:\n","        # count the frequency of each attribute\n","        value_counts = df[col].value_counts()\n","        # create a bar chart of the frequency of each attribute\n","        plt.figure()\n","        plt.bar(value_counts.index, value_counts.values)\n","        plt.title(col)\n","        plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fos5jrWEEmfV"},"outputs":[],"source":["def print_uniq_val(df):\n","    # Create an empty dictionary to store unique values for each column\n","    unique_values_dict = {}\n","\n","    # Iterate through each column of the dataframe\n","    for column_name in df.columns:\n","        # Get the unique values of the current column\n","        unique_values = df[column_name].unique().tolist()\n","\n","        # Sort the unique values\n","        unique_values.sort()\n","\n","        # Store the sorted unique values in the dictionary\n","        unique_values_dict[column_name] = unique_values\n","\n","    # Print the unique values for each column\n","    for column_name, unique_values in unique_values_dict.items():\n","        print(f\"Column name: {column_name}\")\n","        print(f\"Unique values: {unique_values}\")\n","        print(f\"Total unique values: {len(unique_values)}\")\n","        print(\"\\n\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"prglb3bqZ86t"},"outputs":[],"source":["def random_sample(df, n, random_state, replace=False):\n","    \"\"\"\n","    Returns a tuple of two dataframes:\n","    the first dataframe contains n randomly sampled rows from the input dataframe without replacement,\n","    the second dataframe contains the remaining rows that were not sampled.\n","    \"\"\"\n","    sample_df = df.sample(n=n, replace=replace, random_state=random_state)\n","    remaining_df = df.drop(sample_df.index)\n","\n","    return sample_df, remaining_df\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vw9mQljfdTU9"},"outputs":[],"source":["## Standardization\n","# transform numerical features (normalization)\n","def transform_continuous(dataset, continuous_cols):\n","    \"\"\"\n","    dataset: Raw dataset not yet normalized\n","\n","    continuous_cols: a list of continuous columns\n","    \"\"\"\n","\n","    # scaler = MinMaxScaler(feature_range = (0, 1))\n","\n","    scaler = skp.StandardScaler()\n","\n","    columns_to_scale = continuous_cols\n","    scaled_array = scaler.fit_transform(dataset.loc[:, columns_to_scale])\n","    df_scaled = pd.DataFrame(scaled_array, columns=scaler.get_feature_names_out())\n","    return df_scaled\n","\n","\n","# inverse transform normalized vectors\n","def inverse_transform_continuous(original_dataset, synthetic_dataset, col_list):\n","    # scaler = skp.MinMaxScaler(feature_range = (0, 1))\n","    scaler = skp.StandardScaler()\n","    columns_to_normalize = col_list\n","    scaler.fit(original_dataset[columns_to_normalize])\n","    fake_attribute = scaler.inverse_transform(synthetic_dataset[scaler.get_feature_names_out()])\n","    df = pd.DataFrame(fake_attribute, columns = columns_to_normalize)\n","\n","    return df"]},{"cell_type":"markdown","metadata":{"id":"wmaHFbbLIClQ"},"source":["# Load train, test, validation sets"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lWlmiQxHIHsQ"},"outputs":[],"source":["test_df = pd.read_csv(TEST_DF_PATH)\n","train_df = pd.read_csv(TRAIN_DF_PATH)\n","valid_df = pd.read_csv(VALID_DF_PATH)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4,"status":"ok","timestamp":1719906421757,"user":{"displayName":"Phattranit Phattharajiranan","userId":"00695574895698891028"},"user_tz":-420},"id":"rotScY1zPQkK","outputId":"f0832fc6-563a-49f0-dfc8-515c60603013"},"outputs":[{"output_type":"stream","name":"stdout","text":["test: 60451, train: 435254, valid: 108814\n"]}],"source":["print(f\"test: {len(test_df)}, train: {len(train_df)}, valid: {len(valid_df)}\")"]},{"cell_type":"markdown","metadata":{"id":"QZE7AvwujVBU"},"source":["# Network Components"]},{"cell_type":"markdown","metadata":{"id":"xJ-852j-NTYJ"},"source":["## Head"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mF9JxI3JMkmA"},"outputs":[],"source":["class Head(nn.Module):\n","    \"\"\" one head of self-attention \"\"\"\n","\n","    def __init__(self, head_size):\n","        super().__init__()\n","        self.key = nn.Linear(n_embd, head_size, bias=False)\n","        self.query = nn.Linear(n_embd, head_size, bias=False)\n","        self.value = nn.Linear(n_embd, head_size, bias=False)\n","        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n","        self.dropout = nn.Dropout(dropout)\n","\n","    def forward(self, x):\n","        B,T,C = x.shape\n","        k = self.key(x)   # (B,T,C)\n","        q = self.query(x) # (B,T,C)\n","        # compute attention scores (\"affinities\")\n","        wei = q @ k.transpose(-2,-1) * C**-0.5 # (B, T, C) @ (B, C, T) -> (B, T, T)\n","        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n","        wei = F.softmax(wei, dim=-1) # (B, T, T)\n","        wei = self.dropout(wei)\n","        # perform the weighted aggregation of the values\n","        v = self.value(x) # (B,T,C)\n","        out = wei @ v # (B, T, T) @ (B, T, C) -> (B, T, C)\n","        return out"]},{"cell_type":"markdown","metadata":{"id":"I93J81x0NVMH"},"source":["## Multi-head Attention"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZkCSuPYkM0fX"},"outputs":[],"source":["class MultiHeadAttention(nn.Module):\n","    \"\"\" multiple heads of self-attention in parallel \"\"\"\n","\n","    def __init__(self, num_heads, head_size):\n","        super().__init__()\n","        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n","        self.proj = nn.Linear(n_embd, n_embd)\n","        self.dropout = nn.Dropout(dropout)\n","\n","    def forward(self, x):\n","        out = torch.cat([h(x) for h in self.heads], dim=-1)\n","        out = self.dropout(self.proj(out))\n","        return out"]},{"cell_type":"markdown","metadata":{"id":"2EMA_n0SNYm2"},"source":["## Feed forward"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CK49Ct-uM35T"},"outputs":[],"source":["class FeedForward(nn.Module):\n","    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n","\n","    def __init__(self, n_embd):\n","        super().__init__()\n","        self.net = nn.Sequential(\n","            nn.Linear(n_embd, 4 * n_embd),\n","            nn.ReLU(),\n","            nn.Linear(4 * n_embd, n_embd),\n","            nn.Dropout(dropout),\n","        )\n","\n","    def forward(self, x):\n","        return self.net(x)"]},{"cell_type":"markdown","metadata":{"id":"dyFfUa2nNpZh"},"source":["## Block"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KRD3oJ1UM8Lh"},"outputs":[],"source":["class Block(nn.Module):\n","    \"\"\" Transformer block: communication followed by computation \"\"\"\n","\n","    def __init__(self, n_embd, n_head):\n","        # n_embd: embedding dimension, n_head: the number of heads we'd like\n","        super().__init__()\n","        head_size = n_embd // n_head\n","        self.sa = MultiHeadAttention(n_head, head_size)\n","        self.ffwd = FeedForward(n_embd)\n","        self.ln1 = nn.LayerNorm(n_embd)\n","        self.ln2 = nn.LayerNorm(n_embd)\n","\n","    def forward(self, x):\n","        x = x + self.sa(self.ln1(x))\n","        x = x + self.ffwd(self.ln2(x))\n","        return x"]},{"cell_type":"markdown","metadata":{"id":"Z-Ou4Qv_Nrhg"},"source":["## Bigram Model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mIsU0ZO3M_p0"},"outputs":[],"source":["# very simple bigram model\n","class BigramModel(nn.Module):\n","\n","    def __init__(self):\n","        super().__init__()\n","        # each token directly reads off the logits for the next token\n","        # from a lookup table\n","        self.token_embedding_table = nn.Embedding(max_vocab+1, n_embd)\n","        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n","        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n","        self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n","        self.lm_head = nn.Linear(n_embd, max_vocab+1)\n","\n","    def forward(self, idx, targets=None):\n","        B, T = idx.shape\n","\n","        # idx and targets are both (B,T) tensor of integers\n","        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n","\n","        if debug == True:\n","            print(f\"tok_emb: {tok_emb}\")\n","\n","        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n","        x = tok_emb + pos_emb # (B,T,C)\n","        x = self.blocks(x) # (B,T,C)\n","        x = self.ln_f(x) # (B,T,C)\n","        logits = self.lm_head(x) # (B,T,vocab_size)\n","\n","        if targets is None:\n","            loss = None\n","        else:\n","            B, T, C = logits.shape\n","            logits = logits.view(B*T, C)\n","            targets = targets.view(B*T)\n","            if debug == True:\n","                print(f\"logits: {logits}\")\n","                print(f\"targets: {targets}\")\n","                print(f\"logits shape: {logits.shape}\")\n","                print(f\"targets shape: {targets.shape}\")\n","            loss = F.cross_entropy(logits, targets)\n","\n","        return logits, loss\n","\n","    def generate(self, idx, max_new_tokens):\n","        # idx is (B, T) array of indices in the current context\n","        for _ in range(max_new_tokens):\n","            # crop idx to the last block_size tokens\n","            idx_cond = idx[:, -block_size:]\n","            # get the predictions\n","            logits, loss = self(idx_cond)\n","            # focus only on the last time step\n","            logits = logits[:, -1, :] # becomes (B, C)\n","            # apply softmax to get probabilities\n","            probs = F.softmax(logits, dim=-1) # (B, C)\n","            # sample from the distribution\n","            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n","            # append sampled index to the running sequence\n","            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n","        return idx"]},{"cell_type":"markdown","metadata":{"id":"J2V9oI0xQNzt"},"source":["# Load model"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5707,"status":"ok","timestamp":1719906435454,"user":{"displayName":"Phattranit Phattharajiranan","userId":"00695574895698891028"},"user_tz":-420},"id":"CEpdeArVz_He","outputId":"3b7bf1bb-894f-4006-879b-0cac78ef6fc0"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["<All keys matched successfully>"]},"metadata":{},"execution_count":29}],"source":["# Load model\n","model = BigramModel()\n","model = model.to(device)\n","\n","# if using CUDA\n","# model.load_state_dict(torch.load(MODEL_PATH))\n","\n","# if using CPU only\n","model.load_state_dict(torch.load(MODEL_PATH, map_location=torch.device('cpu')))\n","\n","\n","# model.eval()"]},{"cell_type":"markdown","metadata":{"id":"yG8jTFdhN3Cs"},"source":["# Generate from the model"]},{"cell_type":"markdown","metadata":{"id":"UTpUTgQ6Kkfo"},"source":["## Sequentially generate the data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FowN_lZo0NLE"},"outputs":[],"source":["def generate_data_sequentially(size, block_size, initialized_token=0, output_path='output.csv'):\n","    context = torch.tensor(([[initialized_token]]), dtype=torch.long, device=device)\n","    r = (block_size * size)\n","    output = model.generate(context, max_new_tokens=r)[0].tolist()\n","    np.savetxt(output_path, output, delimiter = ',', fmt='%i')\n","    print(f\"output_path: {output_path}, generated output length: {len(output)}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wx6F3Etd0N4d","executionInfo":{"status":"ok","timestamp":1719906862108,"user_tz":-420,"elapsed":186284,"user":{"displayName":"Phattranit Phattharajiranan","userId":"00695574895698891028"}},"outputId":"a854a3c7-a16f-47ce-835f-eabcb5164210"},"outputs":[{"output_type":"stream","name":"stdout","text":["output_path: generated_tokens/tfm.csv, generated output length: 4001\n","CPU times: user 2min 58s, sys: 413 ms, total: 2min 58s\n","Wall time: 3min 6s\n"]}],"source":["%%time\n","generate_data_sequentially(\n","    size=GENERATED_SIZE,\n","    block_size=block_size,\n","    output_path=\"generated_tokens/tfm.csv\",\n",")"]}],"metadata":{"colab":{"collapsed_sections":["7jIbxTJbEjpG","QZE7AvwujVBU","J2V9oI0xQNzt"],"toc_visible":true,"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}